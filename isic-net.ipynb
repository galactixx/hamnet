{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISIC-Net (ResNet50) — HAM10000 + BCN20000\n",
    "- **Goal**: Binary dermoscopic image classification with a fine-tuned ResNet50.\n",
    "- **Datasets**: HAM10000 and BCN20000 combined, with metadata features (sex, age, anatomical site).\n",
    "- **Backbone**: ImageNet-pretrained ResNet50; custom head + metadata MLP.\n",
    "- **Result (test)**: **Accuracy 88.1%**. See the classification report cell for precision/recall/F1 and the confusion matrix cell for error patterns.\n",
    "- **How to use**: Run cells sequentially to prepare data, train, evaluate, and save weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and dependencies\n",
    "Imports core libraries (PyTorch, TorchVision, NumPy, Pandas, etc.), configures deterministic behavior for reproducibility, and silences non-critical warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data paths\n",
    "Configure dataset base directories for HAM10000 and BCN20000. Update paths if running outside Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:12.226699Z",
     "iopub.status.busy": "2025-08-12T15:00:12.226123Z",
     "iopub.status.idle": "2025-08-12T15:00:23.476238Z",
     "shell.execute_reply": "2025-08-12T15:00:23.475592Z",
     "shell.execute_reply.started": "2025-08-12T15:00:12.226672Z"
    },
    "id": "QfLfCneoZvw6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data schema and label mappings\n",
    "Defines `ISICImage` data class and mappings for sex, diagnosis (binary target), and anatomical site. These mappings are used both for dataset construction and metadata normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:23.478044Z",
     "iopub.status.busy": "2025-08-12T15:00:23.477694Z",
     "iopub.status.idle": "2025-08-12T15:00:23.481867Z",
     "shell.execute_reply": "2025-08-12T15:00:23.481189Z",
     "shell.execute_reply.started": "2025-08-12T15:00:23.478024Z"
    },
    "id": "dsQ_fJYNapGz",
    "outputId": "716c5bb4-5b0d-4fdd-df67-5c972c516f70",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_dir = Path(\"/kaggle/input/\")\n",
    "ham_dir = base_dir / \"ham10000\" / \"ISIC-images\"\n",
    "bcn_dir = base_dir / \"bcn20000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata loading and filtering\n",
    "- Concatenates HAM10000 + BCN20000 metadata, de-duplicates by `isic_id`.\n",
    "- Filters to usable records and constructs `ISICImage` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:23.483006Z",
     "iopub.status.busy": "2025-08-12T15:00:23.482752Z",
     "iopub.status.idle": "2025-08-12T15:00:23.596792Z",
     "shell.execute_reply": "2025-08-12T15:00:23.596034Z",
     "shell.execute_reply.started": "2025-08-12T15:00:23.482989Z"
    },
    "id": "JYofYBhdCRgt",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and transforms\n",
    "- Image pipeline: resize-with-pad to 600x450, normalization with ImageNet stats, basic augmentation on train.\n",
    "- Metadata features: sex, age, anatomical site → normalized per-split.\n",
    "- `__getitem__`: returns `(image_tensor, meta_tensor, label)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:23.598343Z",
     "iopub.status.busy": "2025-08-12T15:00:23.598091Z",
     "iopub.status.idle": "2025-08-12T15:00:23.604133Z",
     "shell.execute_reply": "2025-08-12T15:00:23.603414Z",
     "shell.execute_reply.started": "2025-08-12T15:00:23.598325Z"
    },
    "id": "QDCXKNSLmDhC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ISICImage:\n",
    "    path: Path\n",
    "    identifier: str\n",
    "    age: str\n",
    "    sex: str\n",
    "    diagnosis: str\n",
    "    anatom_site: str\n",
    "\n",
    "SEX_MAPPING = {\n",
    "    \"male\": 0,\n",
    "    \"female\": 1\n",
    "}\n",
    "\n",
    "DIAGNOSIS_MAPPING = {\n",
    "    \"Malignant\": 0,\n",
    "    \"Benign\": 1\n",
    "}\n",
    "\n",
    "ANATOM_SITE_MAPPING = {\n",
    "    \"anterior torso\": 0,\n",
    "    \"posterior torso\": 1,\n",
    "    \"head/neck\": 2,\n",
    "    \"upper extremity\": 3,\n",
    "    \"lower extremity\": 4,\n",
    "    \"palms/soles\": 5,\n",
    "    \"oral/genital\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:25.667028Z",
     "iopub.status.busy": "2025-08-12T15:00:25.666492Z",
     "iopub.status.idle": "2025-08-12T15:00:25.671027Z",
     "shell.execute_reply": "2025-08-12T15:00:25.670475Z",
     "shell.execute_reply.started": "2025-08-12T15:00:25.667005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def concat_metadata(paths: List[Path]) -> pd.DataFrame:\n",
    "    data = pd.DataFrame()\n",
    "    for path in paths:\n",
    "        metadata = pd.read_csv(path / \"metadata.csv\")\n",
    "        metadata[\"base_path\"] = path.as_posix()\n",
    "        data = pd.concat([data, metadata])\n",
    "    data = data.drop_duplicates([\"isic_id\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/val/test split and loaders\n",
    "- Stratified split on the binary target.\n",
    "- DataLoaders with deterministic shuffling for train, smaller workers for val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:26.184621Z",
     "iopub.status.busy": "2025-08-12T15:00:26.183918Z",
     "iopub.status.idle": "2025-08-12T15:00:26.189676Z",
     "shell.execute_reply": "2025-08-12T15:00:26.189037Z",
     "shell.execute_reply.started": "2025-08-12T15:00:26.184595Z"
    },
    "id": "HfCBogpYZxLV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_metadata(metadata: pd.DataFrame) -> List[ISICImage]:\n",
    "    metadata = metadata[pd.notnull(metadata[\"age_approx\"])]\n",
    "    metadata = metadata[pd.notnull(metadata[\"sex\"])]\n",
    "    metadata = metadata[pd.notnull(metadata[\"anatom_site_general\"])]\n",
    "    metadata = metadata[metadata[\"diagnosis_1\"].isin([\"Malignant\", \"Benign\"])]\n",
    "    images: List[ISICImage] = []\n",
    "    for idx, row in metadata.iterrows():\n",
    "        images.append(\n",
    "            ISICImage(\n",
    "                path=Path(row[\"base_path\"]),\n",
    "                identifier=row[\"isic_id\"],\n",
    "                age=row[\"age_approx\"],\n",
    "                sex=row[\"sex\"],\n",
    "                diagnosis=row[\"diagnosis_1\"],\n",
    "                anatom_site=row[\"anatom_site_general\"]\n",
    "            )\n",
    "        )\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone initialization\n",
    "- Load ImageNet-pretrained ResNet50.\n",
    "- Freeze all parameters initially and set BatchNorm layers to eval for stable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:29.346306Z",
     "iopub.status.busy": "2025-08-12T15:00:29.346043Z",
     "iopub.status.idle": "2025-08-12T15:00:29.352880Z",
     "shell.execute_reply": "2025-08-12T15:00:29.352250Z",
     "shell.execute_reply.started": "2025-08-12T15:00:29.346286Z"
    },
    "id": "na4CIQ_nsT42",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_diagnosis_counts_by_sex(images: List[ISICImage]) -> None:\n",
    "    counts_by_sex: dict[int, Counter[int]] = defaultdict(Counter)\n",
    "    for img in images:\n",
    "        counts_by_sex[img.sex][img.diagnosis] += 1\n",
    "\n",
    "    all_diagnoses = sorted({d for ctr in counts_by_sex.values() for d in ctr})\n",
    "    all_sexes = sorted(counts_by_sex)\n",
    "\n",
    "    diag_col = \"Diagnosis\"\n",
    "    sex_cols = [f\"Sex {s}\" for s in all_sexes]\n",
    "\n",
    "    w_diag = max(len(diag_col), *(len(str(d)) for d in all_diagnoses))\n",
    "    w_sex = {\n",
    "        s: max(len(f\"Sex {s}\"), *(len(str(counts_by_sex[s][d])) for d in all_diagnoses))\n",
    "        for s in all_sexes\n",
    "    }\n",
    "\n",
    "    header = f\"{diag_col:<{w_diag}} \" + \" \".join(\n",
    "        f\"| {name:>{w_sex[s]}}\" for s, name in zip(all_sexes, sex_cols)\n",
    "    )\n",
    "    sep = \"-\" * len(header)\n",
    "    print(header)\n",
    "    print(sep)\n",
    "\n",
    "    for d in all_diagnoses:\n",
    "        row = f\"{str(d):<{w_diag}} \" + \" \".join(\n",
    "            f\"| {counts_by_sex[s][d]:>{w_sex[s]}}\" for s in all_sexes\n",
    "        )\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:30.951764Z",
     "iopub.status.busy": "2025-08-12T15:00:30.951039Z",
     "iopub.status.idle": "2025-08-12T15:00:30.957040Z",
     "shell.execute_reply": "2025-08-12T15:00:30.956247Z",
     "shell.execute_reply.started": "2025-08-12T15:00:30.951737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResizeWithPad:\n",
    "    def __init__(self, target_size: Tuple[int, int], fill: int = 0) -> None:\n",
    "        self.target_size = target_size  # (height, width)\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        target_h, target_w = self.target_size\n",
    "        orig_w, orig_h = img.size\n",
    "\n",
    "        # Scale by shorter side\n",
    "        scale = min(target_w / orig_w, target_h / orig_h)\n",
    "        new_w = int(orig_w * scale)\n",
    "        new_h = int(orig_h * scale)\n",
    "        img = img.resize((new_w, new_h), Image.BICUBIC)\n",
    "\n",
    "        # Compute padding\n",
    "        pad_w = target_w - new_w\n",
    "        pad_h = target_h - new_h\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "\n",
    "        # Pad\n",
    "        img = transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom), fill=self.fill)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model head with metadata\n",
    "- Replace `resnet50.fc` with identity to extract features.\n",
    "- `meta_net`: small MLP over 3 meta features (sex, age, site).\n",
    "- `classifier`: concatenates image + meta features → 2-class logits.\n",
    "- `ISICNet`: wrapper module combining components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:31.131458Z",
     "iopub.status.busy": "2025-08-12T15:00:31.131214Z",
     "iopub.status.idle": "2025-08-12T15:00:31.136026Z",
     "shell.execute_reply": "2025-08-12T15:00:31.135330Z",
     "shell.execute_reply.started": "2025-08-12T15:00:31.131440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_mean_std(values: List[Any]) -> Tuple[float, float]:\n",
    "    arr = np.array(values, dtype=np.float32)\n",
    "    return float(arr.mean()), float(arr.std())\n",
    "\n",
    "def normalize_meta(values: Sequence[float], means: Sequence[float], stds: Sequence[float]) -> torch.Tensor:\n",
    "    normed = [(v - m) / s for v, m, s in zip(values, means, stds)]\n",
    "    return torch.tensor(normed, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation helper\n",
    "Batched evaluation returning loss, accuracy, predictions and labels. Used for val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:32.317530Z",
     "iopub.status.busy": "2025-08-12T15:00:32.317227Z",
     "iopub.status.idle": "2025-08-12T15:00:32.326259Z",
     "shell.execute_reply": "2025-08-12T15:00:32.325520Z",
     "shell.execute_reply.started": "2025-08-12T15:00:32.317508Z"
    },
    "id": "JTXP9g1biek4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ISICImageDiagnosisDataset(Dataset):\n",
    "    def __init__(self, images: List[ISICImage], train: bool) -> None:\n",
    "        self.images = images\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        ages = [img.age for img in images]\n",
    "        self.mean_age, self.std_age = compute_mean_std(ages)\n",
    "        \n",
    "        sexes = [SEX_MAPPING[img.sex] for img in images]\n",
    "        self.mean_sex, self.std_sex = compute_mean_std(sexes)\n",
    "\n",
    "        sites = [ANATOM_SITE_MAPPING[img.anatom_site] for img in images]\n",
    "        self.mean_site, self.std_site = compute_mean_std(sites)\n",
    "\n",
    "        if not train:\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                ResizeWithPad((600, 450), fill=(0, 0, 0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(self.mean, self.std),\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                ResizeWithPad((600, 450), fill=(0, 0, 0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(self.mean, self.std)\n",
    "            ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        image = self.images[index]\n",
    "        image_path = image.path / f\"{image.identifier}.jpg\"\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        sex = SEX_MAPPING[image.sex]\n",
    "        diagnosis = DIAGNOSIS_MAPPING[image.diagnosis]\n",
    "        site = ANATOM_SITE_MAPPING[image.anatom_site]\n",
    "        age = float(image.age)\n",
    "    \n",
    "        meta = normalize_meta(\n",
    "            values=[sex, age, site],\n",
    "            means=[self.mean_sex, self.mean_age, self.mean_site],\n",
    "            stds=[self.std_sex, self.std_age, self.std_site]\n",
    "        )\n",
    "        return img, meta, diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance handling\n",
    "Compute per-class weights from training distribution and pass to `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:37.167531Z",
     "iopub.status.busy": "2025-08-12T15:00:37.166885Z",
     "iopub.status.idle": "2025-08-12T15:00:38.761537Z",
     "shell.execute_reply": "2025-08-12T15:00:38.760806Z",
     "shell.execute_reply.started": "2025-08-12T15:00:37.167507Z"
    },
    "id": "JLigdi-UsM2K",
    "outputId": "26d4f9a6-e6fe-415e-ae84-9bdeed7ac73f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis | Sex female | Sex male\n",
      "---------------------------------\n",
      "Benign    |       7521 |     7484\n",
      "Malignant |       4615 |     6097\n"
     ]
    }
   ],
   "source": [
    "metadata = concat_metadata(paths=[bcn_dir, ham_dir])\n",
    "images = load_metadata(metadata=metadata)\n",
    "print_diagnosis_counts_by_sex(images=images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training strategy\n",
    "- Stage 1: train only metadata MLP + classifier (frozen backbone) with OneCycleLR.\n",
    "- Stage 2: unfreeze `layer4` and continue fine-tuning with reduced LRs.\n",
    "- Early stopping on best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:40.129231Z",
     "iopub.status.busy": "2025-08-12T15:00:40.128937Z",
     "iopub.status.idle": "2025-08-12T15:00:40.948545Z",
     "shell.execute_reply": "2025-08-12T15:00:40.947665Z",
     "shell.execute_reply.started": "2025-08-12T15:00:40.129208Z"
    },
    "id": "FuZOK8yEZxOb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = [DIAGNOSIS_MAPPING[img.diagnosis] for img in images]\n",
    "\n",
    "train_images, temp_images, _, temp_labels = train_test_split(\n",
    "    images, labels, test_size=0.3, stratify=labels, random_state=SEED\n",
    ")\n",
    "\n",
    "val_images, test_images, _, _ = train_test_split(\n",
    "    temp_images, temp_labels, test_size=0.5, stratify=temp_labels, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:53.872677Z",
     "iopub.status.busy": "2025-08-12T15:00:53.872277Z",
     "iopub.status.idle": "2025-08-12T15:00:53.900063Z",
     "shell.execute_reply": "2025-08-12T15:00:53.899352Z",
     "shell.execute_reply.started": "2025-08-12T15:00:53.872656Z"
    },
    "id": "tzcp8OWrhe2b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = ISICImageDiagnosisDataset(train_images, train=True)\n",
    "test = ISICImageDiagnosisDataset(test_images, train=False)\n",
    "val = ISICImageDiagnosisDataset(val_images, train=False)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "trainloader = DataLoader(train, shuffle=True, batch_size=64, generator=g, num_workers=6)\n",
    "testloader = DataLoader(test, shuffle=False, batch_size=32, num_workers=2)\n",
    "valloader = DataLoader(val, shuffle=False, batch_size=32, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:55.776398Z",
     "iopub.status.busy": "2025-08-12T15:00:55.776038Z",
     "iopub.status.idle": "2025-08-12T15:00:56.932366Z",
     "shell.execute_reply": "2025-08-12T15:00:56.931712Z",
     "shell.execute_reply.started": "2025-08-12T15:00:55.776373Z"
    },
    "id": "yY0uqkIJZxWD",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 176MB/s] \n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import ResNet50_Weights\n",
    "resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:56.933646Z",
     "iopub.status.busy": "2025-08-12T15:00:56.933399Z",
     "iopub.status.idle": "2025-08-12T15:00:56.937799Z",
     "shell.execute_reply": "2025-08-12T15:00:56.936887Z",
     "shell.execute_reply.started": "2025-08-12T15:00:56.933628Z"
    },
    "id": "mqSOKI4Gwf7M",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for name, param in resnet50.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test evaluation\n",
    "Reports final test loss and accuracy. See the next cells for confusion matrix and scikit-learn classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:57.398539Z",
     "iopub.status.busy": "2025-08-12T15:00:57.398260Z",
     "iopub.status.idle": "2025-08-12T15:00:57.402621Z",
     "shell.execute_reply": "2025-08-12T15:00:57.402055Z",
     "shell.execute_reply.started": "2025-08-12T15:00:57.398517Z"
    },
    "id": "scEwsqmaDnmv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for name, module in resnet50.named_modules():\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:00:58.454891Z",
     "iopub.status.busy": "2025-08-12T15:00:58.454405Z",
     "iopub.status.idle": "2025-08-12T15:00:58.475584Z",
     "shell.execute_reply": "2025-08-12T15:00:58.474765Z",
     "shell.execute_reply.started": "2025-08-12T15:00:58.454868Z"
    },
    "id": "CRcn7kuKzCP3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_features = resnet50.fc.in_features\n",
    "resnet50.fc = torch.nn.Identity()\n",
    "\n",
    "meta_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 8),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_features + 8, 1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.4),\n",
    "    torch.nn.Linear(1024, 2)\n",
    ")\n",
    "\n",
    "class ISICNet(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.resnet = resnet50\n",
    "        self.meta_net = meta_net\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, img: torch.Tensor, meta: torch.Tensor) -> torch.Tensor:\n",
    "        img_features = self.resnet(img)\n",
    "        meta_features = self.meta_net(meta)\n",
    "        x = torch.cat([img_features, meta_features], dim=1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T20:40:18.273363Z",
     "iopub.status.busy": "2025-08-12T20:40:18.272494Z",
     "iopub.status.idle": "2025-08-12T20:40:18.279990Z",
     "shell.execute_reply": "2025-08-12T20:40:18.279269Z",
     "shell.execute_reply.started": "2025-08-12T20:40:18.273331Z"
    },
    "id": "c1UycLd0Q0WS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def evaluate(\n",
    "    model: ResNet,\n",
    "    loader: DataLoader,\n",
    "    criterion: CrossEntropyLoss\n",
    ") -> Tuple[int, int, List[np.ndarray], List[np.ndarray]]:\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (imgs, meta, labels) in tqdm(loader, desc=f\"Evaluation: \"):\n",
    "            imgs, meta, labels = imgs.to(device), meta.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(imgs, meta)\n",
    "            loss = criterion(logits, labels)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    eval_loss = running_loss / len(loader.dataset)\n",
    "    return eval_loss, correct / total, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "Visualizes per-class errors on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:01:01.052161Z",
     "iopub.status.busy": "2025-08-12T15:01:01.051632Z",
     "iopub.status.idle": "2025-08-12T15:01:01.301223Z",
     "shell.execute_reply": "2025-08-12T15:01:01.300347Z",
     "shell.execute_reply.started": "2025-08-12T15:01:01.052139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_counts = Counter(DIAGNOSIS_MAPPING[image.diagnosis] for image in train_images)\n",
    "classes = len(label_counts)\n",
    "\n",
    "num_labels = sum(label_counts.values())\n",
    "label_weights = []\n",
    "for idx, (_, count) in enumerate(label_counts.items()):\n",
    "    weight = num_labels / (classes * count)\n",
    "    label_weights.append(weight)\n",
    "\n",
    "weights = torch.tensor(label_weights, dtype=torch.float32)\n",
    "weights = weights.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report\n",
    "Full precision/recall/F1 by class and overall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T15:01:01.314002Z",
     "iopub.status.busy": "2025-08-12T15:01:01.313245Z",
     "iopub.status.idle": "2025-08-12T15:01:01.318975Z",
     "shell.execute_reply": "2025-08-12T15:01:01.318143Z",
     "shell.execute_reply.started": "2025-08-12T15:01:01.313976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def unfreeze_layers(model: ISICNet, layers: List[str]) -> None:\n",
    "    for name, param in model.resnet.named_parameters():\n",
    "        for layer in layers:\n",
    "            if name.startswith(layer):\n",
    "                param.requires_grad = True\n",
    "\n",
    "    for name, module in model.resnet.named_modules():\n",
    "        for layer in layers:\n",
    "            if name.startswith(layer) and isinstance(module, torch.nn.BatchNorm2d):\n",
    "                module.train()\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-12T15:01:02.398169Z",
     "iopub.status.busy": "2025-08-12T15:01:02.397744Z",
     "iopub.status.idle": "2025-08-12T20:03:25.258457Z",
     "shell.execute_reply": "2025-08-12T20:03:25.257614Z",
     "shell.execute_reply.started": "2025-08-12T15:01:02.398144Z"
    },
    "id": "ojyNNIgGahJ6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b1c4c6a3-a604-4d19-b6aa-b50e58dd4a0e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 282/282 [06:38<00:00,  1.41s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:27<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100.. Train loss: 0.544.. Val loss: 0.509.. Accuracy: 0.756..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 282/282 [06:21<00:00,  1.35s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100.. Train loss: 0.531.. Val loss: 0.485.. Accuracy: 0.764..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 282/282 [06:22<00:00,  1.36s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100.. Train loss: 0.543.. Val loss: 0.522.. Accuracy: 0.748..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 282/282 [06:21<00:00,  1.35s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100.. Train loss: 0.548.. Val loss: 0.510.. Accuracy: 0.751..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 282/282 [06:22<00:00,  1.35s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100.. Train loss: 0.527.. Val loss: 0.513.. Accuracy: 0.741..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 282/282 [06:21<00:00,  1.35s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100.. Train loss: 0.509.. Val loss: 0.459.. Accuracy: 0.771..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 282/282 [06:21<00:00,  1.35s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100.. Train loss: 0.496.. Val loss: 0.452.. Accuracy: 0.763..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 282/282 [06:22<00:00,  1.36s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100.. Train loss: 0.487.. Val loss: 0.453.. Accuracy: 0.780..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 282/282 [06:22<00:00,  1.35s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100.. Train loss: 0.471.. Val loss: 0.450.. Accuracy: 0.779..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 282/282 [06:22<00:00,  1.36s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100.. Train loss: 0.471.. Val loss: 0.450.. Accuracy: 0.780..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 282/282 [08:27<00:00,  1.80s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100.. Train loss: 0.467.. Val loss: 0.438.. Accuracy: 0.795..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 282/282 [07:49<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100.. Train loss: 0.440.. Val loss: 0.405.. Accuracy: 0.805..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 282/282 [07:49<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100.. Train loss: 0.419.. Val loss: 0.392.. Accuracy: 0.815..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 282/282 [07:49<00:00,  1.67s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100.. Train loss: 0.395.. Val loss: 0.388.. Accuracy: 0.833..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 282/282 [07:49<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100.. Train loss: 0.368.. Val loss: 0.356.. Accuracy: 0.835..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 282/282 [07:49<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100.. Train loss: 0.345.. Val loss: 0.401.. Accuracy: 0.829..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 282/282 [07:49<00:00,  1.67s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100.. Train loss: 0.315.. Val loss: 0.372.. Accuracy: 0.831..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 282/282 [07:49<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100.. Train loss: 0.293.. Val loss: 0.337.. Accuracy: 0.858..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 282/282 [07:49<00:00,  1.67s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100.. Train loss: 0.276.. Val loss: 0.351.. Accuracy: 0.853..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 282/282 [07:49<00:00,  1.67s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100.. Train loss: 0.253.. Val loss: 0.366.. Accuracy: 0.852..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 282/282 [07:49<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100.. Train loss: 0.242.. Val loss: 0.343.. Accuracy: 0.851..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 282/282 [07:49<00:00,  1.67s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100.. Train loss: 0.223.. Val loss: 0.387.. Accuracy: 0.834..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 282/282 [07:49<00:00,  1.67s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100.. Train loss: 0.220.. Val loss: 0.336.. Accuracy: 0.869..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 282/282 [07:49<00:00,  1.67s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100.. Train loss: 0.203.. Val loss: 0.344.. Accuracy: 0.869..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 282/282 [07:49<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100.. Train loss: 0.192.. Val loss: 0.329.. Accuracy: 0.876..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 282/282 [07:48<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100.. Train loss: 0.187.. Val loss: 0.358.. Accuracy: 0.868..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 282/282 [07:48<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100.. Train loss: 0.182.. Val loss: 0.355.. Accuracy: 0.861..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 282/282 [07:48<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100.. Train loss: 0.170.. Val loss: 0.411.. Accuracy: 0.859..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 282/282 [07:48<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100.. Train loss: 0.160.. Val loss: 0.625.. Accuracy: 0.843..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 282/282 [07:48<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100.. Train loss: 0.149.. Val loss: 0.439.. Accuracy: 0.869..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 282/282 [07:48<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100.. Train loss: 0.143.. Val loss: 0.383.. Accuracy: 0.846..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 282/282 [07:49<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100.. Train loss: 0.129.. Val loss: 0.445.. Accuracy: 0.875..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 282/282 [07:47<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100.. Train loss: 0.119.. Val loss: 0.385.. Accuracy: 0.883..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 282/282 [07:49<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100.. Train loss: 0.114.. Val loss: 0.360.. Accuracy: 0.874..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 282/282 [07:48<00:00,  1.66s/it]\n",
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model = ISICNet()\n",
    "\n",
    "EPOCHS = 100\n",
    "STAGE_1_EPOCHS = 10\n",
    "\n",
    "no_improve = 0\n",
    "patience, best_loss = 10, float('inf')\n",
    "\n",
    "param_groups_1 = [\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 1e-2, \"weight_decay\": 1e-3},\n",
    "    {\"params\": model.meta_net.parameters(), \"lr\": 1e-2, \"weight_decay\": 1e-3},\n",
    "]\n",
    "\n",
    "optimizer = AdamW(param_groups_1)\n",
    "criterion = CrossEntropyLoss(weight=weights)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=[1e-2, 1e-2],\n",
    "    epochs=STAGE_1_EPOCHS,                \n",
    "    steps_per_epoch=len(trainloader),\n",
    "    pct_start=0.25,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(0, STAGE_1_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, meta, labels in tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        imgs, meta, labels = imgs.to(device), meta.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs, meta)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        scheduler.step()\n",
    "\n",
    "    val_loss, val_acc, _, _ = evaluate(model, valloader, criterion)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve == patience:\n",
    "            break\n",
    "\n",
    "    train_loss = running_loss / len(trainloader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}.. \"\n",
    "          f\"Train loss: {train_loss:.3f}.. \"\n",
    "          f\"Val loss: {val_loss:.3f}.. \"\n",
    "          f\"Accuracy: {val_acc:.3f}..\")\n",
    "\n",
    "param_groups_2 = [\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 1e-3, \"weight_decay\": 1e-3},\n",
    "    {\"params\": model.meta_net.parameters(), \"lr\": 1e-3, \"weight_decay\": 1e-3},\n",
    "    {\"params\": model.resnet.layer4.parameters(), \"lr\": 5e-4, \"weight_decay\": 1e-3},\n",
    "]\n",
    "\n",
    "optimizer = AdamW(param_groups_2)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=[1e-3, 1e-3, 5e-4],\n",
    "    epochs=EPOCHS - STAGE_1_EPOCHS,                \n",
    "    steps_per_epoch=len(trainloader),\n",
    "    pct_start=0.25,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "unfreeze_layers(model=model, layers=[\"layer4\"])\n",
    "\n",
    "for epoch in range(STAGE_1_EPOCHS, EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, meta, labels in tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        imgs, meta, labels = imgs.to(device), meta.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs, meta)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        scheduler.step()\n",
    "\n",
    "    val_loss, val_acc, _, _ = evaluate(model, valloader, criterion)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve == patience:\n",
    "            break\n",
    "\n",
    "    train_loss = running_loss / len(trainloader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}.. \"\n",
    "          f\"Train loss: {train_loss:.3f}.. \"\n",
    "          f\"Val loss: {val_loss:.3f}.. \"\n",
    "          f\"Accuracy: {val_acc:.3f}..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-12T20:40:22.436818Z",
     "iopub.status.busy": "2025-08-12T20:40:22.436285Z",
     "iopub.status.idle": "2025-08-12T20:41:34.551838Z",
     "shell.execute_reply": "2025-08-12T20:41:34.551019Z",
     "shell.execute_reply.started": "2025-08-12T20:40:22.436792Z"
    },
    "id": "khN2cnRbUF1I",
    "outputId": "9f11316c-fec2-4eb5-ad0f-bbe1871a573b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 121/121 [01:12<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.350.. Accuracy: 0.881..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_preds, test_labels = evaluate(model, testloader, criterion)\n",
    "print(f\"Test loss: {test_loss:.3f}.. \"\n",
    "      f\"Accuracy: {test_acc:.3f}..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T20:45:04.836122Z",
     "iopub.status.busy": "2025-08-12T20:45:04.835766Z",
     "iopub.status.idle": "2025-08-12T20:45:04.997323Z",
     "shell.execute_reply": "2025-08-12T20:45:04.996663Z",
     "shell.execute_reply.started": "2025-08-12T20:45:04.836091Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T20:46:20.300743Z",
     "iopub.status.busy": "2025-08-12T20:46:20.300164Z",
     "iopub.status.idle": "2025-08-12T20:46:20.314857Z",
     "shell.execute_reply": "2025-08-12T20:46:20.314136Z",
     "shell.execute_reply.started": "2025-08-12T20:46:20.300712Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86      1607\n",
      "           1       0.90      0.90      0.90      2251\n",
      "\n",
      "    accuracy                           0.88      3858\n",
      "   macro avg       0.88      0.88      0.88      3858\n",
      "weighted avg       0.88      0.88      0.88      3858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T20:14:54.889331Z",
     "iopub.status.busy": "2025-08-12T20:14:54.888670Z",
     "iopub.status.idle": "2025-08-12T20:14:55.056843Z",
     "shell.execute_reply": "2025-08-12T20:14:55.056047Z",
     "shell.execute_reply.started": "2025-08-12T20:14:54.889302Z"
    },
    "id": "j6gmm0Jjg2Cl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"ham-net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8051289,
     "sourceId": 12737132,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8051325,
     "sourceId": 12737188,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
