{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QfLfCneoZvw6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from collections import defaultdict, Counter\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsQ_fJYNapGz",
        "outputId": "c3ea32e3-cf66-48cc-90e1-64b6cfa0e2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/ISIC-images/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JYofYBhdCRgt"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QDCXKNSLmDhC"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class HAM10000Image:\n",
        "  identifier: str\n",
        "  age: int\n",
        "  sex: int\n",
        "  diagnosis: int\n",
        "\n",
        "SEX_MAPPING = {\n",
        "  \"male\": 0,\n",
        "  \"female\": 1\n",
        "}\n",
        "\n",
        "DIAGNOSIS_MAPPING = {\n",
        "  \"Malignant\": 0,\n",
        "  \"Benign\": 1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HfCBogpYZxLV"
      },
      "outputs": [],
      "source": [
        "def load_metadata(path: Path) -> List[HAM10000Image]:\n",
        "  data = pd.read_csv(path)\n",
        "  data = data[pd.notnull(data[\"age_approx\"])]\n",
        "  data = data[pd.notnull(data[\"sex\"])]\n",
        "  data = data[data[\"diagnosis_1\"].isin([\"Malignant\", \"Benign\"])]\n",
        "  images: List[HAM10000Image] = []\n",
        "  for idx, row in data.iterrows():\n",
        "    images.append(\n",
        "        HAM10000Image(\n",
        "          identifier=row[\"isic_id\"],\n",
        "          age=row[\"age_approx\"],\n",
        "          sex=row[\"sex\"],\n",
        "          diagnosis=row[\"diagnosis_1\"]\n",
        "      )\n",
        "    )\n",
        "  return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "na4CIQ_nsT42"
      },
      "outputs": [],
      "source": [
        "def print_diagnosis_counts_by_sex(images: List[HAM10000Image]) -> None:\n",
        "    counts_by_sex: dict[int, Counter[int]] = defaultdict(Counter)\n",
        "    for img in images:\n",
        "        counts_by_sex[img.sex][img.diagnosis] += 1\n",
        "\n",
        "    all_diagnoses = sorted({d for ctr in counts_by_sex.values() for d in ctr})\n",
        "    all_sexes = sorted(counts_by_sex)\n",
        "\n",
        "    diag_col = \"Diagnosis\"\n",
        "    sex_cols = [f\"Sex {s}\" for s in all_sexes]\n",
        "\n",
        "    w_diag = max(len(diag_col), *(len(str(d)) for d in all_diagnoses))\n",
        "    w_sex = {\n",
        "        s: max(len(f\"Sex {s}\"), *(len(str(counts_by_sex[s][d])) for d in all_diagnoses))\n",
        "        for s in all_sexes\n",
        "    }\n",
        "\n",
        "    header = f\"{diag_col:<{w_diag}} \" + \" \".join(\n",
        "        f\"| {name:>{w_sex[s]}}\" for s, name in zip(all_sexes, sex_cols)\n",
        "    )\n",
        "    sep = \"-\" * len(header)\n",
        "    print(header)\n",
        "    print(sep)\n",
        "\n",
        "    for d in all_diagnoses:\n",
        "        row = f\"{str(d):<{w_diag}} \" + \" \".join(\n",
        "            f\"| {counts_by_sex[s][d]:>{w_sex[s]}}\" for s in all_sexes\n",
        "        )\n",
        "        print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JTXP9g1biek4"
      },
      "outputs": [],
      "source": [
        "class HAM10000DiagnosisDataset(Dataset):\n",
        "  def __init__(self, images: List[HAM10000Image], train: bool) -> None:\n",
        "    self.images = images\n",
        "    self.mean = [0.485, 0.456, 0.406]\n",
        "    self.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    ages = np.array([img.age for img in images])\n",
        "    self.mean_age, self.std_age = ages.mean(), ages.std()\n",
        "\n",
        "    if not train:\n",
        "      self.transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(self.mean, self.std),\n",
        "      ])\n",
        "    else:\n",
        "      self.transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(self.mean, self.std)\n",
        "      ])\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
        "    image = self.images[index]\n",
        "    image_path = Path(data_dir) / f\"{image.identifier}.jpg\"\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = self.transforms(img)\n",
        "\n",
        "    sex = SEX_MAPPING[image.sex]\n",
        "    diagnosis = DIAGNOSIS_MAPPING[image.diagnosis]\n",
        "    age = int(image.age)\n",
        "\n",
        "    sex = (sex - 0.5) * 2\n",
        "    age = (age - self.mean_age) / self.std_age\n",
        "    meta = torch.tensor([sex, age], dtype=torch.float32)\n",
        "    return img, meta, diagnosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLigdi-UsM2K",
        "outputId": "661cc812-0ac2-43c1-b453-03e56541e7cc"
      },
      "outputs": [],
      "source": [
        "images = load_metadata(Path(data_dir) / \"metadata.csv\")\n",
        "print_diagnosis_counts_by_sex(images=images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "FuZOK8yEZxOb"
      },
      "outputs": [],
      "source": [
        "random.shuffle(images)\n",
        "\n",
        "TRAIN_SPLIT = 0.8\n",
        "TEST_SPLIT = 0.1\n",
        "\n",
        "num_images = len(images)\n",
        "\n",
        "train_index = int(num_images * TRAIN_SPLIT)\n",
        "test_index = train_index + int(num_images * TEST_SPLIT)\n",
        "\n",
        "train_images = images[:train_index]\n",
        "test_images = images[train_index:test_index]\n",
        "val_images = images[test_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "tzcp8OWrhe2b"
      },
      "outputs": [],
      "source": [
        "train = HAM10000DiagnosisDataset(train_images, train=True)\n",
        "test = HAM10000DiagnosisDataset(test_images, train=False)\n",
        "val = HAM10000DiagnosisDataset(val_images, train=False)\n",
        "trainloader = DataLoader(train, shuffle=False, batch_size=64, num_workers=6)\n",
        "testloader = DataLoader(test, shuffle=False, batch_size=32, num_workers=2)\n",
        "valloader = DataLoader(val, shuffle=False, batch_size=32, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "yY0uqkIJZxWD"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import ResNet50_Weights\n",
        "resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "collapsed": true,
        "id": "mqSOKI4Gwf7M"
      },
      "outputs": [],
      "source": [
        "for name, param in resnet50.named_parameters():\n",
        "  if name.startswith(\"layer4\"):\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "scEwsqmaDnmv"
      },
      "outputs": [],
      "source": [
        "for name, module in resnet50.named_modules():\n",
        "  if not name.startswith(\"layer4\"):\n",
        "    if isinstance(module, torch.nn.BatchNorm2d):\n",
        "        module.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "CRcn7kuKzCP3"
      },
      "outputs": [],
      "source": [
        "num_features = resnet50.fc.in_features\n",
        "resnet50.fc = torch.nn.Identity()\n",
        "\n",
        "meta_net = torch.nn.Sequential(\n",
        "  torch.nn.Linear(2, 16),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(16, 8),\n",
        "  torch.nn.ReLU()\n",
        ")\n",
        "\n",
        "classifier = torch.nn.Sequential(\n",
        "  torch.nn.Linear(num_features + 8, 1024),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Dropout(0.1),\n",
        "  torch.nn.Linear(1024, 128),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Dropout(0.1),\n",
        "  torch.nn.Linear(128, 2)\n",
        ")\n",
        "\n",
        "class HAMNet(torch.nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "    self.resnet = resnet50\n",
        "    self.meta_net = meta_net\n",
        "    self.classifier = classifier\n",
        "\n",
        "  def forward(self, img: torch.Tensor, meta: torch.Tensor) -> torch.Tensor:\n",
        "    img_features = self.resnet(img)\n",
        "    meta_features = self.meta_net(meta)\n",
        "    x = torch.cat([img_features, meta_features], dim=1)\n",
        "    return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "c1UycLd0Q0WS"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.resnet import ResNet\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "def evaluate(\n",
        "  model: ResNet,\n",
        "  loader: DataLoader,\n",
        "  criterion: CrossEntropyLoss\n",
        ") -> Tuple[int, int]:\n",
        "  model.eval()\n",
        "  correct = total = 0\n",
        "  running_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for (imgs, meta, labels) in loader:\n",
        "      imgs, meta, labels = imgs.to(device), meta.to(device), labels.to(device)\n",
        "\n",
        "      logits = model(imgs, meta)\n",
        "      loss = criterion(logits, labels)\n",
        "      preds = logits.argmax(dim=1)\n",
        "\n",
        "      correct += (preds == labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "\n",
        "      running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "  eval_loss = running_loss / len(loader.dataset)\n",
        "  return eval_loss, correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "collapsed": true,
        "id": "ojyNNIgGahJ6",
        "outputId": "e23b96c5-efbe-46b1-f704-b98ce679c9a4"
      },
      "outputs": [],
      "source": [
        "model = HAMNet()\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=0.01,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "no_improve = 0\n",
        "patience, best_loss = 7, float('inf')\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.01,\n",
        "    steps_per_epoch=len(trainloader),\n",
        "    epochs=EPOCHS,\n",
        "    pct_start=0.3,\n",
        "    anneal_strategy='cos',\n",
        "    div_factor=15.0,\n",
        "    final_div_factor=1e3,\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for imgs, meta, labels in tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "    imgs, meta, labels = imgs.to(device), meta.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(imgs, meta)\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "  val_loss, val_acc = evaluate(model, valloader, criterion)\n",
        "\n",
        "  if val_loss < best_loss:\n",
        "    best_loss = val_loss\n",
        "    no_improve = 0\n",
        "  else:\n",
        "    no_improve += 1\n",
        "    if no_improve == patience:\n",
        "      break\n",
        "\n",
        "  train_loss = running_loss / len(trainloader.dataset)\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{EPOCHS}.. \"\n",
        "        f\"Train loss: {train_loss:.3f}.. \"\n",
        "        f\"Val loss: {val_loss:.3f}.. \"\n",
        "        f\"Accuracy: {val_acc:.3f}..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khN2cnRbUF1I",
        "outputId": "52958c7a-481c-43a7-ecfc-4cc10a9321a2"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = evaluate(model, testloader, criterion)\n",
        "print(f\"Test loss: {test_loss:.3f}.. \"\n",
        "      f\"Accuracy: {test_acc:.3f}..\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
